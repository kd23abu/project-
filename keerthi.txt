# -*- coding: utf-8 -*-
"""complete ER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13gStHcqde_C1F0s1NLmxOXdohfcSFmZZ

# Electric Vehicle Range Prediction - Regression Analysis

# Introduction

For drivers of electric vehicles (EVs), range is a primary focus. As EVs become more popular, drivers want to know not just “How far can I go?” but also “How can I go further?”  EV range, put simply, is the distance a car can travel on a single charge.

In this notebook, I explore the EV cars dataset and develop Regression to try estimate EV range.
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing the Libraries and data
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from numpy import asarray
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

"""# 1. Load Data"""

# Import the CSV file
# Print the Top five rows of the dataset
ev = pd.read_csv('ElectricCarData_Clean.csv')
ev.head()

"""# 2.Explore Data (Exploratory Data Analysis)"""

#Check available features
ev.columns

#Full summary of the dataset
ev.info()

#Check the number of rows and columns
ev.shape

#Check missing value
ev.isnull().sum()

"""There exists no null value"""

#Check data type of each column
ev.dtypes

#Check the unique values of the dataset
ev.nunique()

#Descriptive Statistics of the dataset
ev.describe()

#Analysis range by EV Brand
ax= plt.figure(figsize=(20,5))
sns.barplot(x='Brand',y='Range_Km',data=ev,palette='hls')
plt.grid(axis='y')
plt.title('Electric Vehicle Range VS EV Brand')
plt.xlabel('Brand')
plt.ylabel('Range per Km')
plt.xticks(rotation=45)

"""Lightyear, Lucid and Tesla have the highest Range and Smart the lowest

# Model with highest Range
"""

range_df = ev.sort_values(by=['Range_Km'], ascending=False)
range_df[['Brand','Model','Range_Km']].head(n=1)

#Analysis acceleration by EV Brand
ax= plt.figure(figsize=(20,5))
sns.barplot(x='Brand',y='AccelSec',data=ev,palette='coolwarm')
plt.grid(axis='y')
plt.title('Electric Vehicle Acceleration VS EV Brand')
plt.xlabel('Brand')
plt.ylabel('AccelSec')
plt.xticks(rotation=45)

"""Lucid, Tesla and Porsche are ranking in Top 3, they can accelerate to 60 mph (97 km/h) at very short time and Renault needs the longest time.

# Model with the shortest Acceleration time
"""

df = ev.sort_values(by=['AccelSec'], ascending=True)
df[['Brand','Model','AccelSec']].head(n=1)

#Analysis top speed by EV Brand
ax= plt.figure(figsize=(20,5))
sns.barplot(x='Brand',y='TopSpeed_KmH',data=ev,palette='husl')
plt.grid(axis='y')
plt.title('Electric Vehicle Top Speed VS EV Brand')
plt.xlabel('Brand')
plt.ylabel('TopSpeed_KmH')
plt.xticks(rotation=45)

"""Telsa, Porsche and Lucid have the highest Speed and Seat the lowest

# Model with Top Speed
"""

speed_df = ev.sort_values(by=['TopSpeed_KmH'], ascending=False)
speed_df[['Brand','Model','TopSpeed_KmH']].head(n=1)

#Analysis efficiency by EV Brand
ax= plt.figure(figsize=(20,5))
sns.barplot(x='Brand',y='Efficiency_WhKm',data=ev,palette='Paired')
plt.grid(axis='y')
plt.title('Electric Vehicle efficiency VS EV Brand')
plt.xlabel('Brand')
plt.ylabel('Efficiency_WhKm')
plt.xticks(rotation=45)

"""Mercedes, Audi and Byton have the highest efficiency and Lightyear the lowest

# Model with maximum Efficiency
"""

eff_df = ev.sort_values(by=['Efficiency_WhKm'], ascending=False)
eff_df[['Brand','Model','Efficiency_WhKm']].head(n=1)

"""# Model with the highest Range & maximum seats"""

seat_df = ev.sort_values(by=['Seats'], ascending=False)
seat_df[['Brand','Model','Range_Km', 'Seats']].head(n=1)

##Distribution of range with PowerTrain
fig, axs = plt.subplots(1,2)
sns.catplot(x="PowerTrain", y="Range_Km", data=ev)
plt.close(1)

"""# 3. Encoding categorical data"""

ev.columns

ev

ev1 = ev[['Brand', 'Model', 'PowerTrain', 'RapidCharge', 'PlugType', 'BodyStyle', 'Segment']]

print(ev1)

# define ordinal encoding
encoder = OrdinalEncoder()

# transform data
result = encoder.fit_transform(ev1)
print(result)

temp = ['Brand', 'Model', 'PowerTrain', 'RapidCharge', 'PlugType', 'BodyStyle', 'Segment']

ev.drop(temp,axis=1,inplace=True)

result = pd.DataFrame(result)

result.columns=["Brand", "Model", "PowerTrain", 'RapidCharge', "PlugType", "BodyStyle", "Segment"]
result.index+=1

result

ev1 = ev
ev1.index+=1

ev1[['Brand', 'Model', 'PowerTrain', 'RapidCharge', 'PlugType', 'BodyStyle', 'Segment']] = result[['Brand', 'Model', 'PowerTrain', 'RapidCharge', 'PlugType', 'BodyStyle', 'Segment']]
ev

ev['FastCharge_KmH'] = pd.to_numeric(ev['FastCharge_KmH'], errors='coerce')

ev.info()

from sklearn.linear_model import LinearRegression

# Split data into rows with and without missing values
missing = ev[ev['FastCharge_KmH'].isnull()]
not_missing = ev[~ev['FastCharge_KmH'].isnull()]

# Train a regression model
X_train = not_missing[['Range_Km', 'Efficiency_WhKm', 'TopSpeed_KmH']]
y_train = not_missing['FastCharge_KmH']
model = LinearRegression()
model.fit(X_train, y_train)

# Predict missing values
X_test = missing[['Range_Km', 'Efficiency_WhKm', 'TopSpeed_KmH']]
predicted_values = model.predict(X_test)

# Fill missing values
ev.loc[ev['FastCharge_KmH'].isnull(), 'FastCharge_KmH'] = predicted_values

ev.info()

#Check correlation between different features
ev.corr()

#Pairplot show the correlation of the data
sns.pairplot(ev,
             corner = True,
             kind = 'scatter',
             hue = 'Range_Km')
plt.show()

#Heatmap to show the correlation of the data
ax= plt.figure(figsize=(15,8))
sns.heatmap(ev.corr(),linewidths=1,linecolor='white',annot=True)

"""Electric Vehicle Range have strong positive correlation with Top Speed per Km driving. It might be sufficient to predit Electric Vehicle Range and then calculate range in Top Speed per Km.

Electric Vehicle Range have a strong negative correlation with Acceleration per second.

# 4. Feature Selection
"""

#Define the target value(dependant variable) as y
X = ev['TopSpeed_KmH'].values.reshape(-1,1)
y = ev['Range_Km']

X.shape

y.shape

"""# 5. Using Linear Regression Create a Model

Linear Regression model is one of the most common algorithms for the regression task.

Strengths:

Simple implementation Linear Regression is a very simple algorithm that can be implemented very easily to give satisfactory results.

Performance on linearly seperable datasets Linear regression fits linearly seperable datasets almost perfectly and is often used to find the nature of the relationship between variable

Overfitting can be reduced by regularization Overfitting is a situation that arises when a machine learning model fits a dataset very closely and hence captures the noisy data as well.

Weaknesses:

Prone to underfitting Underfitting : A sitiuation that arises when a machine learning model fails to capture the data properly.This typically occurs when the hypothesis function cannot fit the data well.

Sensitive to outliers Outliers of a data set are anomalies or extreme values that deviate from the other data points of the distribution.

Linear Regression assumes that the data is independent Very often the inputs aren't independent of each other and hence any multicollinearity must be removed before applying linear regression.

Assumptions:

Linear relationship: There exists a linear relationship between the independent variable, x, and the dependent variable, y.

Independence: The residuals are independent. In particular, there is no correlation between consecutive residuals in time series data.

Homoscedasticity: The residuals have constant variance at every level of x.

Normality: The residuals of the model are normally distributed.
"""

#Training and Test Data

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

## Check Shape, Sample of Test Train Data
print("X_train : ",X_train.shape)

print("X_test : ",X_test.shape)

print("y_train : ",y_train.shape)

print("y_test : ",y_test.shape)

# Create machine learning Model

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# Initialize the ANN model
ann_model = Sequential()

# Add the input layer and the first hidden layer
ann_model.add(Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)))

# Add the second hidden layer
ann_model.add(Dense(units=32, activation='relu'))

# Add the output layer
ann_model.add(Dense(units=1))

# Compile the ANN model
ann_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')

# Summary of the model
ann_model.summary()

# Train the ANN model
history = ann_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)

# Predict on the test data
y_pred_ann = ann_model.predict(X_test)

# Calculate RMSE and R^2 for the ANN model
rmse_ann = np.sqrt(mean_squared_error(y_test, y_pred_ann))
r2_ann = r2_score(y_test, y_pred_ann)

print(f"ANN RMSE: {rmse_ann}")
print(f"ANN R^2: {r2_ann}")

# Plot the training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Calculate R^2 for Training Data
r2_train_ann = r2_score(y_train, ann_model.predict(X_train))
print("ANN R^2 for Training Data: ", r2_train_ann)

# Calculate R^2 for Testing Data
r2_test_ann = r2_score(y_test, y_pred_ann)
print("ANN R^2 for Testing Data: ", r2_test_ann)

# Calculate MSE for Training Data
mse_train_ann = mean_squared_error(y_train, ann_model.predict(X_train))
print("ANN MSE for Training Data: ", mse_train_ann)

# Calculate MSE for Testing Data
mse_test_ann = mean_squared_error(y_test, y_pred_ann)
print("ANN MSE for Testing Data: ", mse_test_ann)

# Calculate MAE for Training Data
mae_train_ann = mean_absolute_error(y_train, ann_model.predict(X_train))
print("ANN MAE for Training Data: ", mae_train_ann)

# Calculate MAE for Testing Data
mae_test_ann = mean_absolute_error(y_test, y_pred_ann)
print("ANN MAE for Testing Data: ", mae_test_ann)

# Calculate RMSE for Training Data
rmse_train_ann = np.sqrt(mse_train_ann)
print("ANN RMSE for Training Data: ", rmse_train_ann)

# Calculate RMSE for Testing Data
rmse_test_ann = np.sqrt(mse_test_ann)
print("ANN RMSE for Testing Data: ", rmse_test_ann)

print("ANN Evaluation Metrics:")
print(f"R^2 for Training Data: {r2_train_ann}")
print(f"R^2 for Testing Data: {r2_test_ann}")
print(f"MSE for Training Data: {mse_train_ann}")
print(f"MSE for Testing Data: {mse_test_ann}")
print(f"MAE for Training Data: {mae_train_ann}")
print(f"MAE for Testing Data: {mae_test_ann}")
print(f"RMSE for Training Data: {rmse_train_ann}")
print(f"RMSE for Testing Data: {rmse_test_ann}")

"""# Create machine learning Model"""

# Create Linear Model
lr = LinearRegression()

#Model Fitting
lr.fit(X_train, y_train)

#We can output a prediction
y_pred = lr.predict(X_test)
y_pred[0:5]

"""Let's assume we have information of following EV

- AccelSec: 5.8
- TopSpeed_KmH: 248
- Efficiency_WhKm: 200
- FastCharge_KmH: 400
- PowerTrain: AWD
- PlugType: Type 2 CCS
- BodyStyle: Hatchback
- Range_Km: ?
- Seats: 5
"""

#Predict the range of this EV using linear regression model.

# TopSpeed_Kmh = 248

EV_pred = lr.predict([[248]])
print(EV_pred)

"""**This means if an EV has a top speed of 248 km/h, the model predicts its range to be ~488.7 km.**
**The prediction is computed using the equation: y = coef * X + intercept**

**Substituting the values:**
**Range_pred = (2.22589054 * 248) - 63.3161642577993**
**Range_pred  # Output: 488.70469038 km**

We can predict the Range of this sample EV is 488 km.
"""

# Check Coefficient
lr.coef_

"""This means for each 1 km/h increase in top speed, the range increases by ~2.23 km."""

#Check intercept
lr.intercept_

"""**This is the y-intercept, which represents the predicted range if the top speed were 0 km/h.**
**A negative intercept suggests that the model might not be valid at extremely low speeds.**

Let's visualize Range as potential predictor variable of Top Speed
"""

width = 12
height = 10
plt.figure(figsize=(width, height))
sns.regplot(x="TopSpeed_KmH", y="Range_Km", data=ev)
plt.ylim(0,)

"""We can see from this plot that range is positively correlated to Top Speed per Kmh, since the regression slope is positive. If the slope is positive, y increases as x increases,  and the function runs "uphill". When Top_Speed increase, the Range will increase too.

The fitted values for a linear regression model are the predicted values of the outcome variable for the data that is used to fit the model.


Let's look at the distribution of the fitted values that result from the model and compare it to the distribution of the actual values
"""

ax1 = sns.kdeplot(y_test, color="r", label="Actual Value")
sns.kdeplot(y_pred, color="b", label="Fitted Values", ax=ax1)
plt.title('Actual vs Fitted value for Range')
plt.xlabel('Range(in Km)')
plt.ylabel('Proportion of Cars')
plt.show()
plt.close()

"""We can see that the fitted values are reasonably close to the actual values, since the two distributions overlap a bit. However, there is definitely some room for improvement.

# 6. Model Evaluation (Regression Metrics)
"""

# Calculate the score for Training Data
lr.score(X_train, y_train)
print("R2 for Traing Data: ", lr.score(X_train, y_train))

# Calculate the score (R^2 for Regression) for Testing Data
lr.score(X_test, y_test)
print("R2 for Testing Data: ", lr.score(X_test, y_test))

#Calculate Mean Squared Error
mean_squared_error(y_test, y_pred)
print("MSE: ", mean_squared_error(y_test, y_pred))

#Calculate Mean Absolute Error(MAE)
mean_absolute_error(y_test, y_pred)
print("MAE: ",mean_absolute_error(y_test, y_pred))

#Calculate Root Mean Squared Error(RMSE)

print("RMSE: ",np.sqrt(mean_squared_error(y_test,y_pred)))

"""# 7. Use Forward Feature Selection to pick a good model"""

# Predictors
X = ev[['TopSpeed_KmH','Efficiency_WhKm','Segment','Seats','AccelSec','PriceEuro']]

# Target
y = ev['Range_Km']

## Create training and testing subsets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

"""Create a Regression model using Forward Feature Selection by looping over all the features adding one at a time until there are no improvements on the prediction metric ( R2 and AdjustedR2 in this case)."""

## Flag intermediate output

show_steps = True   # for testing/debugging
# show_steps = False  # without showing steps

# Use Forward Feature Selection to pick a good model

# start with no predictors
included = []
# keep track of model and parameters
best = {'feature': '', 'r2': 0, 'a_r2': 0}
# create a model object to hold the modelling parameters
model = LinearRegression()
# get the number of cases in the training data
n = X_train.shape[0]

r2_list = []
adjusted_r2_list = []

while True:
    changed = False

    if show_steps:
        print('')

    # list the features to be evaluated
    excluded = list(set(X.columns) - set(included))

    if show_steps:
        print('(Step) Excluded = %s' % ', '.join(excluded))

    # for each remaining feature to be evaluated
    for new_column in excluded:

        if show_steps:
            print('(Step) Trying %s...' % new_column)
            print('(Step) - Features = %s' % ', '.join(included + [new_column]))

        # fit the model with the Training data
        fit = model.fit(X_train[included + [new_column]], y_train)
        # calculate the score (R^2 for Regression)
        r2 = fit.score(X_train[included + [new_column]], y_train)

        # number of predictors in this model
        k = len(included) + 1
        # calculate the adjusted R^2
        adjusted_r2 = 1 - ( ( (1 - r2) * (n - 1) ) / (n - k - 1) )

        if show_steps:
            print('(Step) - Adjusted R^2: This = %.3f; Best = %.3f' %
                  (adjusted_r2, best['a_r2']))

        # if model improves
        if adjusted_r2 > best['a_r2']:
            # record new parameters
            best = {'feature': new_column, 'r2': r2, 'a_r2': adjusted_r2}
            # flag that found a better model
            changed = True
            if show_steps:
                print('(Step) - New Best!   : Feature = %s; R^2 = %.3f; Adjusted R^2 = %.3f' %
                      (best['feature'], best['r2'], best['a_r2']))
    # END for

    r2_list.append(best['r2'])
    adjusted_r2_list.append(best['a_r2'])

    # if found a better model after testing all remaining features
    if changed:
        # update control details
        included.append(best['feature'])
        excluded = list(set(excluded) - set(best['feature']))
        print('Added feature %-4s with R^2 = %.3f and adjusted R^2 = %.3f' %
              (best['feature'], best['r2'], best['a_r2']))
    else:
        # terminate if no better model
        print('*'*50)
        break

print('')
print('Resulting features:')
print(', '.join(included))

## Display both R^2 and Adjusted R^2

_range = range(1, len(r2_list)+1)

# define chart size
plt.figure(figsize = (10, 5))
# plot each metric
plt.plot(_range, r2_list, label = '$R^2$')
plt.plot(_range, adjusted_r2_list, label = '$Adjusted \: R^2$')
# add some better visualisation
plt.xlabel('Number of Features')
plt.legend()
# output the chart
plt.show()

"""I can improve R2 by selecing different features. Every time we add an independent variable to a model then the R-squared increases.

# 8. Regularisation

Linear regression works by selecting coefficients for each independent variable that minimizes a loss function. However, if the coefficients are too large, it can lead to model over-fitting on the training dataset.

The two most common types of regularization are the Ridge and Lasso.
"""

#Create a Base Model Using Linear Regression
model = LinearRegression()
# Fit
model.fit(X,y)

# Check Coeffiricent
model.coef_

These values indicate how much the predicted Range_Km changes per unit increase in each feature.
Interpretation:
Seats and Segment have the highest positive impact on range.
AccelSec has a negative coefficient, meaning faster acceleration reduces range.
Efficiency_WhKm has a very small impact.

def view_coeff(X, model):
    model_coefs = pd.DataFrame({'variable': X.columns,
                                'coef': model.coef_,
                                'abs_coef': np.abs(model.coef_)})
    model_coefs.sort_values('abs_coef', inplace=True, ascending=False)
    sns.barplot(x="coef", y="variable", data=model_coefs)
# Plot Coefficients
view_coeff(X, model)

"""# 9. Calculate Ridge Regression model"""

## Calculate Ridge Regression model

# create a model object to hold the modelling parameters
ridgemodel = Ridge()

# keep track of the intermediate results for coefficients and errors
coefs = []
errors = []

# create a range of alphas to calculate
ridge_alphas = np.logspace(-6, 6, 200) #lambda in the slides

# Train the model with different regularisation strengths
for a in ridge_alphas:
    ridgemodel.set_params(alpha = a)
    ridgemodel.fit(X, y)
    coefs.append(ridgemodel.coef_)
    errors.append(mean_squared_error(ridgemodel.coef_, model.coef_))

"""Visual Representation of Coefficient of Ridge Model"""

# Display results
plt.figure(figsize = (20, 6))

plt.subplot(121)
ax = plt.gca()
ax.plot(ridge_alphas, coefs)
ax.set_xscale('log')
plt.xlabel('alpha')
plt.ylabel('weights')
plt.title('Ridge coefficients as a function of the regularisation')
plt.axis('tight')

plt.subplot(122)
ax = plt.gca()
ax.plot(ridge_alphas, errors)
ax.set_xscale('log')
plt.xlabel('alpha')
plt.ylabel('error')
plt.title('Coefficient error as a function of the regularisation')
plt.axis('tight')

plt.show()

"""Find an optimal value for Ridge regression alpha using RidgeCV"""

optimal_ridge = RidgeCV(alphas=ridge_alphas, cv=10)
optimal_ridge.fit(X, y)
print('Alpha:', optimal_ridge.alpha_)
print('Score:', optimal_ridge.score(X, y))

optimal_ridge.coef_

# Plot Coefficient
view_coeff(X, optimal_ridge)

rr = Ridge(alpha=0.01)
rr.fit(X_train, y_train)
pred_train_rr= rr.predict(X_train)
print(np.sqrt(mean_squared_error(y_train,pred_train_rr)))
print(r2_score(y_train, pred_train_rr))

pred_test_rr= rr.predict(X_test)
print(np.sqrt(mean_squared_error(y_test,pred_test_rr)))
print(r2_score(y_test, pred_test_rr))

"""The above output shows that the RMSE and R-squared values for the Ridge Regression model on the training data is 68.5 and 70.48 percent, respectively. For the test data, the result for these metrics is 98.54 and 26.9 percent, respectively.

# 10. Calculate Lasso Regression model
"""

## Calculate Lasso Regression model

# create a model object to hold the modelling parameters
lassomodel = Lasso()

# keep track of the intermediate results for coefficients and errors
coefs = []
errors = []

# create a range of alphas to calculate
lasso_alphas = np.logspace(-6, 6, 200) #lambda in the slides

# Train the model with different regularisation strengths
for a in lasso_alphas:
    lassomodel.set_params(alpha = a)
    lassomodel.fit(X, y)
    coefs.append(lassomodel.coef_)
    errors.append(mean_squared_error(lassomodel.coef_, model.coef_))

"""Visual Representation of Coefficient of Lasso Model"""

# Display results
plt.figure(figsize = (20, 6))

plt.subplot(121)
ax = plt.gca()
ax.plot(lasso_alphas, coefs)
ax.set_xscale('log')
plt.xlabel('alpha')
plt.ylabel('weights')
plt.title('Ridge coefficients as a function of the regularisation')
plt.axis('tight')

plt.subplot(122)
ax = plt.gca()
ax.plot(lasso_alphas, errors)
ax.set_xscale('log')
plt.xlabel('alpha')
plt.ylabel('error')
plt.title('Coefficient error as a function of the regularisation')
plt.axis('tight')

plt.show()

"""Find an optimal value for Lasso regression alpha using LassoCV."""

# Find Optimal Lasso Using LassoCV

# create a model object to hold the modelling parameters
optimal_lasso = LassoCV(alphas=lasso_alphas, cv=10)

optimal_lasso.fit(X, y)
print('Alpha:', optimal_lasso.alpha_)
print('Score:', optimal_lasso.score(X, y))

optimal_lasso.coef_

# Plot Coefficient
view_coeff(X, optimal_lasso)

model_lasso = Lasso(alpha=0.01)
model_lasso.fit(X_train, y_train)
pred_train_lasso= model_lasso.predict(X_train)
print(np.sqrt(mean_squared_error(y_train,pred_train_lasso)))
print(r2_score(y_train, pred_train_lasso))

pred_test_lasso= model_lasso.predict(X_test)
print(np.sqrt(mean_squared_error(y_test,pred_test_lasso)))
print(r2_score(y_test, pred_test_lasso))

"""The above output shows that the RMSE and R-squared values for the Lasso Regression model on the training data is 68.5 and 70.5 percent, respectively. The results for these metrics on the test data is 98.5 and 26.89 percent, respectively. Lasso Regression can also be used for feature selection because the coeﬃcients of less important features are reduced to zero.

Compare the residuals for the Ridge and Lasso visually.
"""

# Ridge model residuals
optimal_ridge.fit(X_train, y_train)
prediction = optimal_ridge.predict(X_test)
residual_ridge = (y_test - prediction)
print(residual_ridge)

# Lasso model residuals
optimal_lasso.fit(X_train, y_train)
prediction = optimal_lasso.predict(X_test)
residual_lasso = (y_test - prediction)
print(residual_lasso)

# Jointplot

sns.jointplot(data=ev, x=residual_lasso, y=residual_ridge, kind = 'scatter')

"""# Conclusions

From my analysis, affect EV Range Factors are TopSpeed_KmH, Efficiency_WhKm, AccelSec, Segment, Seats and PriceEuro.

There are a number of factors that also affect EV range:
    
- Your driving style

- Terrain and road condition

- The weather

- The weight of the vehicle

- Using the heating and air conditioning

- Condition of the tyres

- Condition of the battery

Tips to Maximize Range (Example for EV car - Tesla Model Y)

- Slow down your driving and avoid frequent and rapid acceleration.

- If safe to do so, modulate the accelerator pedal instead of using the brake pedal when gradually slowing down.

- Limit the use of resources such as heating and air conditioning.

-  With your vehicle plugged in, use the mobile app to precondition your vehicle to ensure the cabin is at a comfortable temperature and windows are defrosted (if needed) before your drive

- Touch Schedule, available on both the charging and climate control screens, to set a time when you want your vehicle to be ready to drive

- Set Stopping Mode to Hold to gain the benefit of regenerative braking at low driving speeds

- Ensure the wheels are aligned to specification, the tires are kept at the recommended inflation pressures (see Tire Care and Maintenance), and are rotated when needed

- Install aero covers (if equipped) to reduce wind resistance

- Lighten your load by removing any unnecessary cargo.

- Fully raise all windows.

- Features such as Sentry Mode and Cabin Overheat Protection can impact range. Disable features when not needed.

- To prevent an excessive amount of energy consumption while the vehicle is idle, keep the vehicle plugged in when not in use.

- Minimize the use of DC chargers (such as Superchargers) for optimal Battery health.
"""